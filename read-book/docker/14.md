# 容器中的内存与I/O性能问题

## Buffered I/O与Direct I/O

- Linux支持两种I/O模式：Buffered I/O（带缓冲）和Direct I/O（直接I/O）。
- Buffered I/O是默认模式，通常在应用程序中更为普遍。

## 问题描述

- 在虚拟机迁移到容器后，使用Buffered I/O的应用程序可能会遇到写入延时波动较大的问题。

## 问题再现

- 设计一个小程序，模拟从文件读取64KB数据块并写入新文件，记录写入每个数据块的时间。
- 在虚拟机和容器中运行该程序，并比较结果。

## 问题分析

- 怀疑Linux内核对dirty pages的操作可能影响了Buffered I/O的写操作。

### Dirty Pages相关内核参数

- `dirty_background_ratio`：后台刷写阈值，默认10%。
- `dirty_background_bytes`：后台刷写字节数，与`dirty_background_ratio`作用相同，但为字节值。
- `dirty_ratio`：刷写阈值，默认20%，超过此比例会阻塞写操作直到数据刷写完成。
- `dirty_bytes`：与`dirty_ratio`相对应，为字节值。
- `dirty_writeback_centisecs`：内核flush线程处理dirty pages的时间间隔，默认5秒。
- `dirty_expire_centisecs`：dirty page在内存中存放的最长时间，默认30秒。

### 实验验证

- 观察容器中进程写入时的dirty pages数目，使用`watch -n1 "cat /proc/vmstat | grep dirty"`。

### 调试方法

- 使用`perf`和`ftrace`工具对容器内进程进行性能分析。
- 通过`perf record`和`perf report`找出`write`系统调用下的主要子函数。
- 使用`ftrace`的`set_ftrace_filter`和`function_graph`追踪这些子函数的执行时间。

### 调试结果

- 发现`write`系统调用在申请PageCache页面时，会反复调用`mem_cgroup_try_charge`。
- 释放页面时，`do_try_to_free_pages()`函数耗时较长，达到50+微秒。

## 总结

- 在容器环境中，Buffered I/O的写入延时波动可能与Memory Cgroup限制和dirty pages的处理有关。
- 通过调整内核参数和使用性能分析工具，可以定位并解决容器中的I/O性能问题。
- 对于容器中的内存和I/O性能调优，理解内核参数和系统调用行为是关键。
